{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training loop for descriptor \"M_Back\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import pandas as pd\n",
    "from ie590_project_nonGIT.model.input_fn import input_fn_features\n",
    "from ie590_project_nonGIT.utils.utils import Params, build_train_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Using device:  /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# Set up some global variables\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "print('Using device: ', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Train and Validation Sets. Default ratio is 80%train, 20%test\n",
    "#build_train_validate('data_descriptors_mode.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'update': 'params.json',\n",
       " 'learning_rate': 1e-05,\n",
       " 'batch_size': 200,\n",
       " 'num_epochs': 10,\n",
       " 'reg_scale': 0.0001,\n",
       " 'num_ch': 6,\n",
       " 'image_height': 240,\n",
       " 'image_width': 320,\n",
       " 'max_frames': 25,\n",
       " 'image_resize': 240,\n",
       " 'num_parallel_calls': 4,\n",
       " 'save_summary_steps': 1,\n",
       " 'train_path': 'train.csv',\n",
       " 'validation_path': 'validation.csv',\n",
       " 'prediction_path': 'predictions_mode.csv',\n",
       " 'dataset_path': '/home/jupyter/CV_Project/ie590_project/data/IsoGD_phase_1/',\n",
       " 'descriptors': 'M_Back',\n",
       " 'descriptor_type': 'mode'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = Params('params.json')\n",
    "is_training = True\n",
    "params.dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.937834\n",
      "1    0.062166\n",
      "Name: M_Back, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5be34ccd90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD1CAYAAABA+A6aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAKnUlEQVR4nO3cUYideXnH8e/PhLSg1otmKjbJOAGztFEKC0MseFGLW5pVSG6kJFBoy2KuUilKaaSySHpj7YVXEQy0tAg1Tb1oB02bgl2htF2bWdSFJKQd0tUMuTDaVSiiMfbpxUzt8eTMnDfZk5ydZ78fCJz3ff+c8xAmX/5555yTqkKStPO9bt4DSJJmw6BLUhMGXZKaMOiS1IRBl6QmDLokNbF7Xi+8d+/eWlpamtfLS9KO9MILL3yrqhYmXZtb0JeWllhdXZ3Xy0vSjpTk61td85aLJDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6Qm5vbBop1i6cwX5j1CKy99/H3zHkFqyx26JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpiUFBT3I0yY0ka0nOTLi+mOS5JF9J8mKS985+VEnSdqYGPcku4BzwNHAYOJnk8NiyjwIXq+pJ4ATwqVkPKkna3pAd+hFgrapuVtVd4AJwfGxNAT+z+fhNwO3ZjShJGmL3gDX7gFsjx+vAO8fWfAz4hyS/C7weeGom00mSBhuyQ8+EczV2fBL486raD7wX+EyS+547yakkq0lW79y58+DTSpK2NCTo68CBkeP93H9L5RngIkBV/Svw08De8SeqqvNVtVxVywsLCw83sSRpoiFBvwIcSnIwyR42fum5MrbmG8B7AJL8IhtBdwsuSY/R1KBX1T3gNHAZuM7Gu1muJjmb5Njmsg8DH0jyNeCzwG9X1fhtGUnSIzTkl6JU1SXg0ti5Z0ceXwPeNdvRJEkPwk+KSlITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1MSgoCc5muRGkrUkZ7ZY8xtJriW5muQvZzumJGma3dMWJNkFnAN+DVgHriRZqaprI2sOAR8B3lVVLyf5uUc1sCRpsiE79CPAWlXdrKq7wAXg+NiaDwDnquplgKr65mzHlCRNMyTo+4BbI8frm+dGPQE8keSfkzyf5OisBpQkDTP1lguQCedqwvMcAt4N7Af+Kck7quo7P/FEySngFMDi4uIDDytJ2tqQHfo6cGDkeD9we8Kav62qH1bVfwI32Aj8T6iq81W1XFXLCwsLDzuzJGmCIUG/AhxKcjDJHuAEsDK25m+AXwVIspeNWzA3ZzmoJGl7U4NeVfeA08Bl4DpwsaquJjmb5NjmssvAt5NcA54Dfr+qvv2ohpYk3W/IPXSq6hJwaezcsyOPC/jQ5h9J0hz4SVFJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaGBT0JEeT3EiyluTMNuven6SSLM9uREnSEFODnmQXcA54GjgMnExyeMK6NwIfBL486yElSdMN2aEfAdaq6mZV3QUuAMcnrPsj4BPA92c4nyRpoCFB3wfcGjle3zz3Y0meBA5U1ednOJsk6QEMCXomnKsfX0xeB3wS+PDUJ0pOJVlNsnrnzp3hU0qSphoS9HXgwMjxfuD2yPEbgXcAX0ryEvDLwMqkX4xW1fmqWq6q5YWFhYefWpJ0nyFBvwIcSnIwyR7gBLDyfxer6rtVtbeqlqpqCXgeOFZVq49kYknSRFODXlX3gNPAZeA6cLGqriY5m+TYox5QkjTM7iGLquoScGns3LNbrH33Kx9LkvSg/KSoJDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTQwKepKjSW4kWUtyZsL1DyW5luTFJF9M8tbZjypJ2s7UoCfZBZwDngYOAyeTHB5b9hVguap+Cfgc8IlZDypJ2t6QHfoRYK2qblbVXeACcHx0QVU9V1Xf2zx8Htg/2zElSdMMCfo+4NbI8frmua08A/zdKxlKkvTgdg9YkwnnauLC5DeBZeBXtrh+CjgFsLi4OHBESdIQQ3bo68CBkeP9wO3xRUmeAv4QOFZVP5j0RFV1vqqWq2p5YWHhYeaVJG1hSNCvAIeSHEyyBzgBrIwuSPIk8Gk2Yv7N2Y8pSZpmatCr6h5wGrgMXAcuVtXVJGeTHNtc9ifAG4C/TvLVJCtbPJ0k6REZcg+dqroEXBo79+zI46dmPJck6QH5SVFJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITu+c9gKSHs3TmC/MeoZWXPv6+eY/wirlDl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYGBT3J0SQ3kqwlOTPh+k8l+avN619OsjTrQSVJ25sa9CS7gHPA08Bh4GSSw2PLngFerqq3AZ8E/njWg0qStjdkh34EWKuqm1V1F7gAHB9bcxz4i83HnwPekySzG1OSNM2QL+faB9waOV4H3rnVmqq6l+S7wM8C3xpdlOQUcGrz8L+T3HiYoTXRXsb+vl+N4v/dXov82Zytt251YUjQJ+206yHWUFXngfMDXlMPKMlqVS3Pew5pnD+bj8+QWy7rwIGR4/3A7a3WJNkNvAn4r1kMKEkaZkjQrwCHkhxMsgc4AayMrVkBfmvz8fuBf6yq+3bokqRHZ+otl8174qeBy8Au4M+q6mqSs8BqVa0Afwp8JskaGzvzE49yaE3krSy9Wvmz+ZjEjbQk9eAnRSWpCYMuSU0YdElqYsj70PUqk+QX2Ph07j423u9/G1ipqutzHUzSXLlD32GS/AEbX78Q4N/YeFtpgM9O+uI06dUiye/Me4bufJfLDpPk34G3V9UPx87vAa5W1aH5TCZtL8k3qmpx3nN05i2Xned/gJ8Hvj52/i2b16S5SfLiVpeANz/OWV6LDPrO83vAF5P8B///pWmLwNuA03ObStrwZuDXgZfHzgf4l8c/zmuLQd9hqurvkzzBxtca72PjH8o6cKWqfjTX4ST4PPCGqvrq+IUkX3r847y2eA9dkprwXS6S1IRBl6QmDLokNWHQJakJgy5JTfwvm2YZv37xTLAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptor = params.descriptors\n",
    "df = pd.read_csv(params.train_path,usecols=[descriptor])\n",
    "dist = df[descriptor].value_counts(normalize=True)\n",
    "print(dist)\n",
    "dist.plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight imbalance in the distribution. Must Evaluate by AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick between channel='rgb'/'d'/'both'\n",
    "class VGG16_FC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size_1, hidden_size_2, num_classes=2, channels='both'):\n",
    "        super(VGG16_FC, self).__init__()        \n",
    "        #self.input_layer = tf.keras.layers.InputLayer(input_shape=num_features*25),\n",
    "        self.channels = channels\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size_1\n",
    "                                         , activation=tf.nn.leaky_relu)\n",
    "        self.fc2 = tf.keras.layers.Dense(hidden_size_2\n",
    "                                         , activation=tf.nn.leaky_relu)\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes\n",
    "                                         , activation='softmax')\n",
    "        if self.channels == 'rgb':\n",
    "            self.ix = 0\n",
    "        else:\n",
    "            self.ix = 1\n",
    "    \n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if self.channels == 'both':\n",
    "            x = np.concatenate([x[:,0,:,:],x[:,1,:,:]],axis=2)\n",
    "        else:\n",
    "            x = x[:,self.ix,:,:]\n",
    "        frame_features = []\n",
    "        for i in range(x.shape[1]):\n",
    "            frame_features.append(x[:,i,:])\n",
    "        x = np.concatenate(frame_features,axis=1)\n",
    "        #x = self.input_layer(x)\n",
    "        x = self.fc1(x)      \n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VGG16_LSTM(tf.keras.Model):\n",
    "    def __init__(self, num_features=512, hidden_size_1=128, num_classes=2, channels='both', dropout=0.2):\n",
    "        super(VGG16_LSTM, self).__init__()        \n",
    "        #self.input_layer = tf.keras.layers.InputLayer(input_shape=num_features*25),\n",
    "        self.channels = channels\n",
    "        if self.channels == 'both':\n",
    "            num_features *= 2\n",
    "            hidden_size_1 *= 2\n",
    "        self.lstm = tf.keras.layers.LSTM(num_features, return_sequences=False,\n",
    "                                               input_shape=(25,),\n",
    "                                               dropout=dropout)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size_1,activation=tf.nn.leaky_relu)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes,activation='softmax')\n",
    "        if channels == 'rgb':\n",
    "            self.ix = 0\n",
    "        else:\n",
    "            self.ix = 1\n",
    "    \n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if self.channels == 'both':\n",
    "            x = np.concatenate([x[:,0,:,:],x[:,1,:,:]],axis=2)\n",
    "        else:\n",
    "            x = x[:,self.ix,:,:]\n",
    "        #x = self.input_layer(x)\n",
    "        x = self.lstm(x)      \n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#layers = [tf.compat.v2.keras.layers.InputLayer(12800),\n",
    "          #tf.compat.v2.keras.layers.Dense(2000, activation='relu'),\n",
    "          #tf.compat.v2.keras.layers.Dense(200, activation='relu'),\n",
    "          #tf.compat.v2.keras.layers.Dense(2, activation='softmax')\n",
    "         #]\n",
    "#model = tf.keras.Sequential(layers)\n",
    "\n",
    "def model_init_fn():\n",
    "    model = VGG16_FC(hidden_size_1=2000, hidden_size_2=200, \n",
    "                     num_classes=2, channels='both')\n",
    "    return model\n",
    "\n",
    "def model_init_fn_lstm():\n",
    "    model = VGG16_LSTM(channels='both')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data from: train.csv\n",
      "0    8071\n",
      "1    5649\n",
      "Name: M_Back, dtype: int64\n",
      "Loading Validation Data from: validation.csv\n"
     ]
    }
   ],
   "source": [
    "train = input_fn_features(is_training=True,params=params,up_sample=True)\n",
    "val = input_fn_features(is_training=False,params=params)\n",
    "#{images, targets, iterator.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn,is_training,compile_params):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Model, History (Loss, AUC, Recall, Accuracy)\n",
    "    \"\"\"    \n",
    "    with tf.device(compile_params['device']):\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "        \n",
    "        model = model_init_fn()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=compile_params[\"learning_rate\"])\n",
    "        #Calculate Loss\n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        #Area Under Curve \n",
    "        train_auc = tf.keras.metrics.AUC(name='train_auc')\n",
    "        val_auc = tf.keras.metrics.AUC(name='val_auc')\n",
    "        #Recall\n",
    "        train_recall = tf.keras.metrics.Recall(name='train_recall')\n",
    "        val_recall = tf.keras.metrics.Recall(name='val_recall')\n",
    "        #Accuracy\n",
    "        train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "        val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n",
    "        \n",
    "        #Logging all the above for plots later on\n",
    "        train_loss_log = []\n",
    "        val_loss_log = []\n",
    "        train_auc_log = []\n",
    "        val_auc_log = []\n",
    "        train_recall_log = []\n",
    "        val_recall_log = []\n",
    "        train_accuracy_log = []\n",
    "        val_accuracy_log = []\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(compile_params[\"num_epochs\"]):\n",
    "            train_dset = iter(compile_params['train_data'])\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "            train_recall.reset_states()\n",
    "            train_auc.reset_states()\n",
    "    \n",
    "            \n",
    "            for x_np, y_np in train_dset:\n",
    "                y_np = tf.keras.utils.to_categorical(y_np, num_classes = 2, dtype = 'int32')\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=True)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_auc.update_state(y_np, scores)\n",
    "                    train_recall.update_state(y_np, scores)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "                    \n",
    "                    if t % compile_params[\"print_every\"] == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "                        val_recall.reset_states()\n",
    "                        val_auc.reset_states()\n",
    "                        \n",
    "                        val_dset = iter(compile_params['val_data'])\n",
    "                        \n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            test_y = tf.keras.utils.to_categorical(test_y, num_classes = 2)\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "                            \n",
    "                            val_loss.update_state(t_loss)\n",
    "                            sparse_prediction = tf.keras.utils.to_categorical(np.argmax(prediction,axis=1), num_classes = 2)\n",
    "                            val_auc.update_state(test_y, prediction)\n",
    "                            val_recall.update_state(test_y, sparse_prediction)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "                            #print(\"Debug:\",val_accuracy.result())\n",
    "\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Train Loss: {}, Train {}: {}, Val Loss: {}, Val {}: {}'\n",
    "                        print (template.format(t, epoch+1, \n",
    "                                             train_loss.result(),\n",
    "                                            compile_params['metric'],\n",
    "                                             train_accuracy.result(),\n",
    "                                             val_loss.result(),\n",
    "                                            compile_params['metric'],\n",
    "                                             val_accuracy.result()))\n",
    "                    t += 1\n",
    "            train_loss_log.append(train_loss.result())\n",
    "            val_loss_log.append(val_loss.result())\n",
    "            train_auc_log.append(train_auc.result())\n",
    "            val_auc_log.append(val_auc.result())\n",
    "            train_recall_log.append(train_recall.result())\n",
    "            val_recall_log.append(val_recall.result())\n",
    "            train_accuracy_log.append(train_accuracy.result())\n",
    "            val_accuracy_log.append(val_accuracy.result())   \n",
    "            history = {\"loss\":{\"train\":train_loss_log,\"validation\":val_loss_log},\n",
    "                      \"auc\":{\"train\":train_auc_log,\"validation\":val_auc_log},\n",
    "                      \"recall\":{\"train\":train_recall_log,\"validation\":val_recall_log},\n",
    "                      \"accuracy\":{\"train\":train_accuracy_log,\"validation\":val_accuracy_log}}\n",
    "        return(model,history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer vg_g16_fc is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Iteration 0, Epoch 1, Train Loss: 0.692747950553894, Train Accuracy: 0.5950000286102295, Val Loss: 6.401262283325195, Val Accuracy: 0.06694560497999191\n",
      "Iteration 20, Epoch 1, Train Loss: 1.010683298110962, Train Accuracy: 0.5649999976158142, Val Loss: 0.4765843152999878, Val Accuracy: 0.9037656784057617\n",
      "Iteration 40, Epoch 1, Train Loss: 0.8015101552009583, Train Accuracy: 0.6251219511032104, Val Loss: 0.3554205894470215, Val Accuracy: 0.918177604675293\n",
      "Iteration 60, Epoch 1, Train Loss: 0.7016276121139526, Train Accuracy: 0.6663934588432312, Val Loss: 0.3273675739765167, Val Accuracy: 0.9079498052597046\n",
      "Iteration 80, Epoch 2, Train Loss: 0.4483165740966797, Train Accuracy: 0.7854166626930237, Val Loss: 0.28381821513175964, Val Accuracy: 0.918642520904541\n",
      "Iteration 100, Epoch 2, Train Loss: 0.43527349829673767, Train Accuracy: 0.7904687523841858, Val Loss: 0.28459596633911133, Val Accuracy: 0.9060901999473572\n",
      "Iteration 120, Epoch 2, Train Loss: 0.41965198516845703, Train Accuracy: 0.8003846406936646, Val Loss: 0.17938297986984253, Val Accuracy: 0.9414225816726685\n",
      "Iteration 140, Epoch 3, Train Loss: 0.35916876792907715, Train Accuracy: 0.8183333277702332, Val Loss: 0.17145222425460815, Val Accuracy: 0.9428172707557678\n",
      "Iteration 160, Epoch 3, Train Loss: 0.3630214333534241, Train Accuracy: 0.8310869336128235, Val Loss: 0.2344738394021988, Val Accuracy: 0.9149233102798462\n",
      "Iteration 180, Epoch 3, Train Loss: 0.3458283841609955, Train Accuracy: 0.8438372015953064, Val Loss: 0.22989754378795624, Val Accuracy: 0.9112040996551514\n",
      "Iteration 200, Epoch 3, Train Loss: 0.33982548117637634, Train Accuracy: 0.8488888740539551, Val Loss: 0.2909000515937805, Val Accuracy: 0.874941885471344\n",
      "Iteration 220, Epoch 4, Train Loss: 0.3051868975162506, Train Accuracy: 0.8682143092155457, Val Loss: 0.32093122601509094, Val Accuracy: 0.8568108081817627\n",
      "Iteration 240, Epoch 4, Train Loss: 0.2853575646877289, Train Accuracy: 0.8852941393852234, Val Loss: 0.23321586847305298, Val Accuracy: 0.9065551161766052\n",
      "Iteration 260, Epoch 4, Train Loss: 0.2807336449623108, Train Accuracy: 0.8867592811584473, Val Loss: 0.23076000809669495, Val Accuracy: 0.9033007621765137\n",
      "Iteration 280, Epoch 5, Train Loss: 0.23100730776786804, Train Accuracy: 0.9210000038146973, Val Loss: 0.2667858898639679, Val Accuracy: 0.8865643739700317\n",
      "Iteration 300, Epoch 5, Train Loss: 0.23869659006595612, Train Accuracy: 0.9121999740600586, Val Loss: 0.14280329644680023, Val Accuracy: 0.9437471032142639\n",
      "Iteration 320, Epoch 5, Train Loss: 0.2424764186143875, Train Accuracy: 0.906000018119812, Val Loss: 0.23949512839317322, Val Accuracy: 0.8972570896148682\n",
      "Iteration 340, Epoch 5, Train Loss: 0.2347581386566162, Train Accuracy: 0.910076916217804, Val Loss: 0.24787093698978424, Val Accuracy: 0.8912134170532227\n",
      "Iteration 360, Epoch 6, Train Loss: 0.19179508090019226, Train Accuracy: 0.9346874952316284, Val Loss: 0.20566128194332123, Val Accuracy: 0.9125987887382507\n",
      "Iteration 380, Epoch 6, Train Loss: 0.1937997043132782, Train Accuracy: 0.9312499761581421, Val Loss: 0.19201380014419556, Val Accuracy: 0.9153881669044495\n",
      "Iteration 400, Epoch 6, Train Loss: 0.1954362541437149, Train Accuracy: 0.9300892949104309, Val Loss: 0.15098027884960175, Val Accuracy: 0.9400278925895691\n",
      "Iteration 420, Epoch 7, Train Loss: 0.18300406634807587, Train Accuracy: 0.9292857050895691, Val Loss: 0.15450669825077057, Val Accuracy: 0.9367735981941223\n",
      "Iteration 440, Epoch 7, Train Loss: 0.16421543061733246, Train Accuracy: 0.9429629445075989, Val Loss: 0.19359154999256134, Val Accuracy: 0.9112040996551514\n",
      "Iteration 460, Epoch 7, Train Loss: 0.15692374110221863, Train Accuracy: 0.9507446885108948, Val Loss: 0.1752321720123291, Val Accuracy: 0.9237564206123352\n",
      "Iteration 480, Epoch 7, Train Loss: 0.15457458794116974, Train Accuracy: 0.9508955478668213, Val Loss: 0.15304051339626312, Val Accuracy: 0.9316596984863281\n",
      "Iteration 500, Epoch 8, Train Loss: 0.14183279871940613, Train Accuracy: 0.9588888883590698, Val Loss: 0.1090371161699295, Val Accuracy: 0.956764280796051\n",
      "Iteration 520, Epoch 8, Train Loss: 0.14239156246185303, Train Accuracy: 0.9544736742973328, Val Loss: 0.18721866607666016, Val Accuracy: 0.9177126884460449\n",
      "Iteration 540, Epoch 8, Train Loss: 0.13649670779705048, Train Accuracy: 0.9554310441017151, Val Loss: 0.2385379672050476, Val Accuracy: 0.892143189907074\n",
      "Iteration 560, Epoch 9, Train Loss: 0.1084241271018982, Train Accuracy: 0.976111114025116, Val Loss: 0.2036253958940506, Val Accuracy: 0.909809410572052\n",
      "Iteration 580, Epoch 9, Train Loss: 0.11608178168535233, Train Accuracy: 0.9648275971412659, Val Loss: 0.14397265017032623, Val Accuracy: 0.9344490766525269\n",
      "Iteration 600, Epoch 9, Train Loss: 0.11038888245820999, Train Accuracy: 0.9685714244842529, Val Loss: 0.11401435732841492, Val Accuracy: 0.9553695917129517\n",
      "Iteration 620, Epoch 9, Train Loss: 0.10732099413871765, Train Accuracy: 0.9701166152954102, Val Loss: 0.11033593863248825, Val Accuracy: 0.9572291970252991\n",
      "Iteration 640, Epoch 10, Train Loss: 0.09338094294071198, Train Accuracy: 0.9742500185966492, Val Loss: 0.10238723456859589, Val Accuracy: 0.9614133238792419\n",
      "Iteration 660, Epoch 10, Train Loss: 0.10881322622299194, Train Accuracy: 0.9642500281333923, Val Loss: 0.37980034947395325, Val Accuracy: 0.8382147550582886\n",
      "Iteration 680, Epoch 10, Train Loss: 0.11818268150091171, Train Accuracy: 0.9584166407585144, Val Loss: 0.13403914868831635, Val Accuracy: 0.9418874979019165\n",
      "Iteration 700, Epoch 11, Train Loss: 0.12752369046211243, Train Accuracy: 0.9431818127632141, Val Loss: 0.1342669278383255, Val Accuracy: 0.9414225816726685\n",
      "Iteration 720, Epoch 11, Train Loss: 0.11098889261484146, Train Accuracy: 0.958548367023468, Val Loss: 0.14942039549350739, Val Accuracy: 0.9325894713401794\n",
      "Iteration 740, Epoch 11, Train Loss: 0.10357383638620377, Train Accuracy: 0.9623529314994812, Val Loss: 0.1037101075053215, Val Accuracy: 0.9590888023376465\n",
      "Iteration 760, Epoch 12, Train Loss: 0.08619031310081482, Train Accuracy: 0.9674999713897705, Val Loss: 0.10236548632383347, Val Accuracy: 0.9614133238792419\n",
      "Iteration 780, Epoch 12, Train Loss: 0.0697275772690773, Train Accuracy: 0.9802272915840149, Val Loss: 0.10134322196245193, Val Accuracy: 0.9600185751914978\n",
      "Iteration 800, Epoch 12, Train Loss: 0.06940947473049164, Train Accuracy: 0.9814285635948181, Val Loss: 0.1042623296380043, Val Accuracy: 0.9614133238792419\n",
      "Iteration 820, Epoch 12, Train Loss: 0.06546901166439056, Train Accuracy: 0.9825000166893005, Val Loss: 0.11321990191936493, Val Accuracy: 0.9562994241714478\n",
      "Iteration 840, Epoch 13, Train Loss: 0.04840737208724022, Train Accuracy: 0.9903846383094788, Val Loss: 0.10556404292583466, Val Accuracy: 0.9576941132545471\n",
      "Iteration 860, Epoch 13, Train Loss: 0.05068713054060936, Train Accuracy: 0.9890909194946289, Val Loss: 0.10115479677915573, Val Accuracy: 0.9618781805038452\n",
      "Iteration 880, Epoch 13, Train Loss: 0.05345305800437927, Train Accuracy: 0.9877358675003052, Val Loss: 0.10140258818864822, Val Accuracy: 0.9628080129623413\n",
      "Iteration 900, Epoch 14, Train Loss: 0.04613199830055237, Train Accuracy: 0.9912499785423279, Val Loss: 0.12209363281726837, Val Accuracy: 0.9530450701713562\n",
      "Iteration 920, Epoch 14, Train Loss: 0.044596944004297256, Train Accuracy: 0.9891666769981384, Val Loss: 0.11116668581962585, Val Accuracy: 0.9549046754837036\n",
      "Iteration 940, Epoch 14, Train Loss: 0.040791887789964676, Train Accuracy: 0.9905681610107422, Val Loss: 0.1175951212644577, Val Accuracy: 0.9521152973175049\n",
      "Iteration 960, Epoch 14, Train Loss: 0.04390450194478035, Train Accuracy: 0.9898437261581421, Val Loss: 0.10656845569610596, Val Accuracy: 0.9623430967330933\n",
      "Iteration 980, Epoch 15, Train Loss: 0.05085977166891098, Train Accuracy: 0.9860000014305115, Val Loss: 0.10812404751777649, Val Accuracy: 0.9600185751914978\n",
      "Iteration 1000, Epoch 15, Train Loss: 0.04738578572869301, Train Accuracy: 0.9885714054107666, Val Loss: 0.10961687564849854, Val Accuracy: 0.9604834914207458\n",
      "Iteration 1020, Epoch 15, Train Loss: 0.04225466027855873, Train Accuracy: 0.9902727007865906, Val Loss: 0.10734046250581741, Val Accuracy: 0.9623430967330933\n"
     ]
    }
   ],
   "source": [
    "compile_params = {\n",
    "    \"device\": device,\n",
    "    \"train_data\": train,\n",
    "    \"val_data\": val,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"num_epochs\": 15,\n",
    "    \"metric\": \"Accuracy\",\n",
    "    \"descriptor_type\": \"mode\",\n",
    "    \"print_every\": 20,\n",
    "}\n",
    "m_back_model_fc, m_back_history_fc = train_part34(model_init_fn,is_training,compile_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer vg_g16_lstm is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Iteration 0, Epoch 1, Train Loss: 0.7027040123939514, Train Accuracy: 0.5849999785423279, Val Loss: 0.652387797832489, Val Accuracy: 0.918177604675293\n",
      "Iteration 20, Epoch 1, Train Loss: 0.6688741445541382, Train Accuracy: 0.5935714244842529, Val Loss: 0.5930333733558655, Val Accuracy: 0.8284518718719482\n",
      "Iteration 40, Epoch 1, Train Loss: 0.6458709239959717, Train Accuracy: 0.6229268312454224, Val Loss: 0.5005074143409729, Val Accuracy: 0.8103207945823669\n",
      "Iteration 60, Epoch 1, Train Loss: 0.6175522208213806, Train Accuracy: 0.650081992149353, Val Loss: 0.37672945857048035, Val Accuracy: 0.8307763934135437\n",
      "Iteration 80, Epoch 2, Train Loss: 0.4642549455165863, Train Accuracy: 0.7712500095367432, Val Loss: 0.2452772706747055, Val Accuracy: 0.8986517786979675\n",
      "Iteration 100, Epoch 2, Train Loss: 0.4563504755496979, Train Accuracy: 0.7762500047683716, Val Loss: 0.4316892623901367, Val Accuracy: 0.7717340588569641\n",
      "Iteration 120, Epoch 2, Train Loss: 0.4449463188648224, Train Accuracy: 0.7825000286102295, Val Loss: 0.44253304600715637, Val Accuracy: 0.7563923597335815\n",
      "Iteration 140, Epoch 3, Train Loss: 0.3623770773410797, Train Accuracy: 0.8483333587646484, Val Loss: 0.24751268327236176, Val Accuracy: 0.883774995803833\n",
      "Iteration 160, Epoch 3, Train Loss: 0.375713586807251, Train Accuracy: 0.8221738934516907, Val Loss: 0.24472841620445251, Val Accuracy: 0.8777312636375427\n",
      "Iteration 180, Epoch 3, Train Loss: 0.3640207350254059, Train Accuracy: 0.8306976556777954, Val Loss: 0.2410900741815567, Val Accuracy: 0.8823803067207336\n",
      "Iteration 200, Epoch 3, Train Loss: 0.3569183349609375, Train Accuracy: 0.8336507678031921, Val Loss: 0.3797069787979126, Val Accuracy: 0.8028823733329773\n",
      "Iteration 220, Epoch 4, Train Loss: 0.33429816365242004, Train Accuracy: 0.8489285707473755, Val Loss: 0.24888992309570312, Val Accuracy: 0.8809856176376343\n",
      "Iteration 240, Epoch 4, Train Loss: 0.3107476830482483, Train Accuracy: 0.8617647290229797, Val Loss: 0.19460700452327728, Val Accuracy: 0.9084146618843079\n",
      "Iteration 260, Epoch 4, Train Loss: 0.31199270486831665, Train Accuracy: 0.8597221970558167, Val Loss: 0.17108875513076782, Val Accuracy: 0.9251511096954346\n",
      "Iteration 280, Epoch 5, Train Loss: 0.4608220160007477, Train Accuracy: 0.7630000114440918, Val Loss: 0.18789447844028473, Val Accuracy: 0.9144583940505981\n",
      "Iteration 300, Epoch 5, Train Loss: 0.3862258195877075, Train Accuracy: 0.8131999969482422, Val Loss: 0.40032675862312317, Val Accuracy: 0.7814970016479492\n",
      "Iteration 320, Epoch 5, Train Loss: 0.35359659790992737, Train Accuracy: 0.8338888883590698, Val Loss: 0.31647640466690063, Val Accuracy: 0.8307763934135437\n",
      "Iteration 340, Epoch 5, Train Loss: 0.3346211314201355, Train Accuracy: 0.8443077206611633, Val Loss: 0.18590165674686432, Val Accuracy: 0.9028359055519104\n",
      "Iteration 360, Epoch 6, Train Loss: 0.2541072964668274, Train Accuracy: 0.8887500166893005, Val Loss: 0.266058087348938, Val Accuracy: 0.8679683804512024\n",
      "Iteration 380, Epoch 6, Train Loss: 0.24958056211471558, Train Accuracy: 0.893750011920929, Val Loss: 0.2657916247844696, Val Accuracy: 0.8768014907836914\n",
      "Iteration 400, Epoch 6, Train Loss: 0.2507685720920563, Train Accuracy: 0.892589271068573, Val Loss: 0.18483246862888336, Val Accuracy: 0.9102742671966553\n",
      "Iteration 420, Epoch 7, Train Loss: 0.21696674823760986, Train Accuracy: 0.9100000262260437, Val Loss: 0.17860271036624908, Val Accuracy: 0.9139934778213501\n",
      "Iteration 440, Epoch 7, Train Loss: 0.22862836718559265, Train Accuracy: 0.9053703546524048, Val Loss: 0.22223316133022308, Val Accuracy: 0.883774995803833\n",
      "Iteration 460, Epoch 7, Train Loss: 0.23158186674118042, Train Accuracy: 0.9031915068626404, Val Loss: 0.3649526536464691, Val Accuracy: 0.8186889886856079\n",
      "Iteration 480, Epoch 7, Train Loss: 0.23393215239048004, Train Accuracy: 0.9003731608390808, Val Loss: 0.2344355583190918, Val Accuracy: 0.8763365745544434\n",
      "Iteration 500, Epoch 8, Train Loss: 0.21679013967514038, Train Accuracy: 0.9130555391311646, Val Loss: 0.17769767343997955, Val Accuracy: 0.9112040996551514\n",
      "Iteration 520, Epoch 8, Train Loss: 0.20934052765369415, Train Accuracy: 0.9115789532661438, Val Loss: 0.33506670594215393, Val Accuracy: 0.8344956040382385\n",
      "Iteration 540, Epoch 8, Train Loss: 0.2219911813735962, Train Accuracy: 0.9054310321807861, Val Loss: 0.15322761237621307, Val Accuracy: 0.9237564206123352\n",
      "Iteration 560, Epoch 9, Train Loss: 0.18661898374557495, Train Accuracy: 0.9222221970558167, Val Loss: 0.13683763146400452, Val Accuracy: 0.9372385144233704\n",
      "Iteration 580, Epoch 9, Train Loss: 0.191677987575531, Train Accuracy: 0.9212068915367126, Val Loss: 0.18929839134216309, Val Accuracy: 0.9042305946350098\n",
      "Iteration 600, Epoch 9, Train Loss: 0.20144391059875488, Train Accuracy: 0.9151020646095276, Val Loss: 0.17742931842803955, Val Accuracy: 0.9153881669044495\n",
      "Iteration 620, Epoch 9, Train Loss: 0.19775176048278809, Train Accuracy: 0.9161078929901123, Val Loss: 0.11859884858131409, Val Accuracy: 0.9497907757759094\n",
      "Iteration 640, Epoch 10, Train Loss: 0.1904735118150711, Train Accuracy: 0.9212499856948853, Val Loss: 0.15158861875534058, Val Accuracy: 0.9242212772369385\n",
      "Iteration 660, Epoch 10, Train Loss: 0.19714128971099854, Train Accuracy: 0.9182500243186951, Val Loss: 0.11530452221632004, Val Accuracy: 0.9497907757759094\n",
      "Iteration 680, Epoch 10, Train Loss: 0.20276027917861938, Train Accuracy: 0.9146666526794434, Val Loss: 0.21804751455783844, Val Accuracy: 0.8893538117408752\n",
      "Iteration 700, Epoch 11, Train Loss: 0.1790364384651184, Train Accuracy: 0.9240909218788147, Val Loss: 0.11526589095592499, Val Accuracy: 0.9474663138389587\n",
      "Iteration 720, Epoch 11, Train Loss: 0.16922715306282043, Train Accuracy: 0.9311290383338928, Val Loss: 0.20358838140964508, Val Accuracy: 0.9005113840103149\n",
      "Iteration 740, Epoch 11, Train Loss: 0.1717199683189392, Train Accuracy: 0.9297058582305908, Val Loss: 0.2027289867401123, Val Accuracy: 0.9056252837181091\n",
      "Iteration 760, Epoch 12, Train Loss: 0.15492525696754456, Train Accuracy: 0.9325000047683716, Val Loss: 0.22414086759090424, Val Accuracy: 0.883774995803833\n",
      "Iteration 780, Epoch 12, Train Loss: 0.16779308021068573, Train Accuracy: 0.9277272820472717, Val Loss: 0.1885722577571869, Val Accuracy: 0.9102742671966553\n",
      "Iteration 800, Epoch 12, Train Loss: 0.16714926064014435, Train Accuracy: 0.9291666746139526, Val Loss: 0.1378737837076187, Val Accuracy: 0.9339842200279236\n",
      "Iteration 820, Epoch 12, Train Loss: 0.1724298596382141, Train Accuracy: 0.9266935586929321, Val Loss: 0.1441795974969864, Val Accuracy: 0.9232915043830872\n",
      "Iteration 840, Epoch 13, Train Loss: 0.1491701751947403, Train Accuracy: 0.9426922798156738, Val Loss: 0.11438004672527313, Val Accuracy: 0.9418874979019165\n",
      "Iteration 860, Epoch 13, Train Loss: 0.15473100543022156, Train Accuracy: 0.9395454525947571, Val Loss: 0.2663678824901581, Val Accuracy: 0.8735471963882446\n"
     ]
    }
   ],
   "source": [
    "compile_params = {\n",
    "    \"device\": device,\n",
    "    \"train_data\": train,\n",
    "    \"val_data\": val,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"num_epochs\": 15,\n",
    "    \"metric\": \"Accuracy\",\n",
    "    \"descriptor_type\": \"mode\",\n",
    "    \"print_every\": 20,\n",
    "}\n",
    "m_back_model_lstm, m_back_history_lstm = train_part34(model_init_fn_lstm,is_training,compile_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(model_plt, title):\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.set_size_inches(15, 10)\n",
    "    axs[0, 0].plot(model_plt['loss']['train'])\n",
    "    axs[0, 0].plot(model_plt['loss']['validation'])\n",
    "    axs[0, 0].set_title('Loss')\n",
    "    axs[0, 0].set(ylabel='Loss')\n",
    "    axs[0, 1].plot(model_plt['accuracy']['train'])\n",
    "    axs[0, 1].plot(model_plt['accuracy']['validation'])\n",
    "    axs[0, 1].set_title('Accuracy')\n",
    "    axs[0, 1].set(ylabel='Accuracy')\n",
    "    axs[1, 0].plot(model_plt['auc']['train'])\n",
    "    axs[1, 0].plot(model_plt['auc']['validation'])\n",
    "    axs[1, 0].set_title('AUC')\n",
    "    axs[1, 0].set(ylabel='AUC')\n",
    "    axs[1, 1].plot(model_plt['recall']['train'])\n",
    "    axs[1, 1].plot(model_plt['recall']['validation'])\n",
    "    axs[1, 1].set_title('Recall')\n",
    "    axs[1, 1].set(ylabel='Recall')\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='Epochs')\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.legend(['train', 'test'], loc='upper left')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "model_plt = m_back_history_fc\n",
    "plot_stuff(model_plt, title = 'Descriptor: M_Back | Model: VGG16-FC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "model_plt = m_back_history_lstm\n",
    "plot_stuff(model_plt, title = 'Descriptor: M_Back | Model: VGG16-LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC Curves\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.xlim([-0.5,20])\n",
    "    plt.ylim([80,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\n",
    "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Models\n",
    "m_back_model_fc.save_weights('/home/jupyter/CV_Project/ie590_project/ie590_project_nonGIT/model/m_back_model_fc')\n",
    "m_back_model_lstm.save_weights('/home/jupyter/CV_Project/ie590_project/ie590_project_nonGIT/model/m_back_model_lstm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbb005ba990>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Models\n",
    "#fc_model_loaded = VGG16_FC(2000,200)\n",
    "#fc_filepath = '/home/jupyter/CV_Project/ie590_project/ie590_project_nonGIT/model/m_back_model_fc'\n",
    "#fc_model_loaded.load_weights(fc_filepath)\n",
    "\n",
    "lstm_model_loaded = VGG16_LSTM()\n",
    "lstm_filepath = '/home/jupyter/CV_Project/ie590_project/ie590_project_nonGIT/model/m_back_model_lstm'\n",
    "lstm_model_loaded.load_weights(lstm_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.descriptor_type = 'mean'\n",
    "def predict_descriptor(model_weights, descriptor, csv_file = 'predictions_mode.csv'):\n",
    "    input_pred = input_fn_features(is_training=False,params=params,prediction_mode=True)\n",
    "    pred_data = iter(input_pred)\n",
    "    predictions = []\n",
    "    for x_pred, __ in pred_data:\n",
    "        preds = model_weights(x_pred, training=False)\n",
    "        preds = np.argmax(preds.numpy(),axis=1)\n",
    "        for i in preds:\n",
    "            predictions.append(i)\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[descriptor] = predictions \n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(\"Updated predictions on %s\"%csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Data from: predictions_mode.csv\n",
      "WARNING:tensorflow:Layer vg_g16_lstm is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Updated predictions on predictions_mode.csv\n"
     ]
    }
   ],
   "source": [
    "predict_descriptor(lstm_model_loaded,'M_Back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
